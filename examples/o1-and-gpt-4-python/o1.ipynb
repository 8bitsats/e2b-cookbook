{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code Interpreting with OpenAI's o1 model\n",
        "This example uses the E2B's [Code Interpreter](https://github.com/e2b-dev/code-interpreter) as a tool for OpenAI's o1 model. We ask o1 to generate Python code to train a machine learning model based on Kaggle's [Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic) challenge and display the results in a graph.\n",
        "\n",
        "We use two LLMs:\n",
        "1. o1-mini to generate a thorough plan for the task with many code blocks\n",
        "2. gpt-4o-mini to extract the final code from the plan generated by o1-mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z7uxglM8rkss"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.54.5)\n",
            "Requirement already satisfied: e2b_code_interpreter==1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.1)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b_code_interpreter==1.0.0) (23.2.0)\n",
            "Requirement already satisfied: e2b<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b_code_interpreter==1.0.0) (1.0.1)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b_code_interpreter==1.0.0) (0.27.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: httpcore<2.0.0,>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (1.0.5)\n",
            "Requirement already satisfied: packaging<25.0,>=24.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (24.1)\n",
            "Requirement already satisfied: protobuf<6.0.0,>=3.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (4.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.20.0->e2b_code_interpreter==1.0.0) (2024.8.30)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore<2.0.0,>=1.0.5->e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->e2b<2.0.0,>=1.0.0->e2b_code_interpreter==1.0.0) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai e2b_code_interpreter==1.0.0 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw165_Rl7go4",
        "outputId": "ee53a14c-752c-413b-c2b0-4c1e3a33de59"
      },
      "outputs": [],
      "source": [
        "# Get your API keys or save them to .env file.\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# TODO: Get your OpenAI API key from https://platform.openai.com/api-keys\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# TODO: Get your E2B API key from https://e2b.dev/docs\n",
        "E2B_API_KEY = os.getenv(\"E2B_API_KEY\")\n",
        "\n",
        "O1_PROMPT = \"\"\"You're a data scientist analyzing survival data from the Titanic Disaster. You are given tasks to complete and you run Python code to solve them.\n",
        "\n",
        "Information about the Titanic dataset:\n",
        "- It's in the `/home/user/train.csv` and `/home/user/test.csv` files\n",
        "- The CSV files are using `,` as the delimiter\n",
        "- They have following columns:\n",
        "  - PassengerId: Unique passenger ID\n",
        "  - Pclass: 1st, 2nd, 3rd (Ticket class)\n",
        "  - Name: Passenger name\n",
        "  - Sex: Gender\n",
        "  - Age: Age in years\n",
        "  - SibSp: Number of siblings/spouses aboard\n",
        "  - Parch: Number of parents/children aboard\n",
        "  - Ticket: Ticket number\n",
        "  - Fare: Passenger fare\n",
        "  - Cabin: Cabin number\n",
        "  - Embarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
        "\n",
        "Generally, you follow these rules:\n",
        "- ALWAYS FORMAT YOUR RESPONSE IN MARKDOWN\n",
        "- ALWAYS RESPOND ONLY WITH CODE IN CODE BLOCK LIKE THIS:\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "- the python code runs in jupyter notebook.\n",
        "- every time you generate python, the code is executed in a separate cell. it's okay to multiple calls to `execute_python`.\n",
        "- display visualizations using matplotlib or any other visualization library directly in the notebook. don't worry about saving the visualizations to a file.\n",
        "- you have access to the internet and can make api requests.\n",
        "- you also have access to the filesystem and can read/write files.\n",
        "- install all packages before using by running `!pip install {package}`.\n",
        "- you can run any python code you want, everything is running in a secure sandbox environment\n",
        "\"\"\"\n",
        "\n",
        "GPT_4O_PROMPT = \"You are an expert software engineer that receives an execution plan, and then creates a single Python script that does everything in the plan. It will be executed in a single Python notebook cell.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UT578ezzr4RN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "pattern = re.compile(r'```python\\n(.*?)\\n```', re.DOTALL)  # Match everything in between ```python and ```\n",
        "\n",
        "def match_code_blocks(llm_response):\n",
        "    matches = pattern.findall(llm_response)  # Find all matches\n",
        "    if matches:\n",
        "        code = \"\\n\".join(matches)  # Concatenate all code blocks with newlines\n",
        "        print(\"> LLM-generated code:\")\n",
        "        print(code)\n",
        "        return code\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wzoaB4yYskGO"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def chat(e2b_code_interpreter, user_message):\n",
        "  print(f\"\\n{'='*50}\\nUser message: {user_message}\\n{'='*50}\")\n",
        "\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "  # First we get the plan from o1\n",
        "  response_o1 = client.chat.completions.create(\n",
        "    model=\"o3-mini\", # Choose different model by uncommenting\n",
        "    # model=\"o1-mini\",\n",
        "    messages=[\n",
        "      {\"role\": \"user\", \"content\": O1_PROMPT},\n",
        "      {\"role\": \"user\", \"content\": user_message}\n",
        "  ])\n",
        "  content_o1 = response_o1.choices[0].message.content\n",
        "\n",
        "  # Then we use gpt-4o-mini to extract the code from the plan we got from o1\n",
        "  response_4o = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": GPT_4O_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"This is the plan I received, please write fully functional code that I can run in one notebook cell, and list all its dependencies: {content_o1}\"},\n",
        "  ])\n",
        "  content_4o = response_4o.choices[0].message.content\n",
        "  python_code = match_code_blocks(content_4o)\n",
        "\n",
        "  if python_code != \"\":\n",
        "    code_interpreter_results = code_interpret(e2b_code_interpreter, python_code)\n",
        "    return code_interpreter_results\n",
        "  else:\n",
        "    print(f\"Failed to match any Python code in model's response {content_4o}\")\n",
        "    return[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wSBB7e1Tsnav"
      },
      "outputs": [],
      "source": [
        "def upload_dataset(code_interpreter):\n",
        "  print(\"Uploading testing and training datasets to Code Interpreter sandbox...\")\n",
        "  file_path = \"./test.csv\"\n",
        "  with open(file_path, \"rb\") as f:\n",
        "    code_interpreter.files.write(file_path, f)\n",
        "    print(\"Uploaded test.csv at\", file_path)\n",
        "  file_path = \"./train.csv\"\n",
        "  with open(file_path, \"rb\") as f:\n",
        "    code_interpreter.files.write(file_path, f)\n",
        "    print(\"Uploaded train.csv at\", file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hibi4_ZluIRb"
      },
      "outputs": [],
      "source": [
        "def code_interpret(e2b_code_interpreter, code):\n",
        "  print(\"Running code interpreter...\")\n",
        "  exec = e2b_code_interpreter.run_code(code,\n",
        "  on_stderr=lambda stderr: print(\"[Code Interpreter]\", stderr),\n",
        "  on_stdout=lambda stdout: print(\"[Code Interpreter]\", stdout))\n",
        "\n",
        "  if exec.error:\n",
        "    print(\"[Code Interpreter ERROR]\", exec.error)\n",
        "  else:\n",
        "    return exec.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b4VsGbZyspXb",
        "outputId": "cf82fa50-355f-438d-e2c5-6adef7e28abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading testing and training datasets to Code Interpreter sandbox...\n",
            "Uploaded test.csv at ./test.csv\n",
            "Uploaded train.csv at ./train.csv\n",
            "\n",
            "==================================================\n",
            "User message: Clean the data, train a decision tree to predict the survival of passengers, and visualize the learning curve. Then run the model the test dataset and print the results.\n",
            "==================================================\n",
            "> LLM-generated code:\n",
            "# Install necessary packages\n",
            "!pip install pandas scikit-learn matplotlib seaborn\n",
            "\n",
            "# Import libraries\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.model_selection import learning_curve, train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "# Set a style for seaborn\n",
            "sns.set(style=\"whitegrid\")\n",
            "\n",
            "# Define a preprocessing function for both training and test data\n",
            "def preprocess_data(df, is_train=True):\n",
            "    drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
            "    if not is_train:\n",
            "        ids = df['PassengerId']\n",
            "        df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
            "    else:\n",
            "        df = df.drop(drop_cols, axis=1)\n",
            "    \n",
            "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
            "    if 'Fare' in df.columns:\n",
            "        df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
            "    if 'Embarked' in df.columns:\n",
            "        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
            "    \n",
            "    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
            "    \n",
            "    if is_train:\n",
            "        y = df['Survived']\n",
            "        X = df.drop('Survived', axis=1)\n",
            "        return X, y\n",
            "    else:\n",
            "        return df, ids\n",
            "\n",
            "# Load the data (please adjust paths accordingly)\n",
            "train_df = pd.read_csv('train.csv', delimiter=',')  # Change path as needed\n",
            "test_df = pd.read_csv('test.csv', delimiter=',')   # Change path as needed\n",
            "\n",
            "# Preprocess the training data\n",
            "X, y = preprocess_data(train_df, is_train=True)\n",
            "\n",
            "# Initialize the Decision Tree Classifier and compute the learning curve\n",
            "clf = DecisionTreeClassifier(random_state=42)\n",
            "\n",
            "train_sizes, train_scores, validation_scores = learning_curve(\n",
            "    estimator=clf,\n",
            "    X=X,\n",
            "    y=y,\n",
            "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
            "    cv=5,\n",
            "    scoring='accuracy',\n",
            "    shuffle=True,\n",
            "    random_state=42\n",
            ")\n",
            "\n",
            "train_mean = np.mean(train_scores, axis=1)\n",
            "train_std = np.std(train_scores, axis=1)\n",
            "validation_mean = np.mean(validation_scores, axis=1)\n",
            "validation_std = np.std(validation_scores, axis=1)\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training score\")\n",
            "plt.plot(train_sizes, validation_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
            "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"r\", alpha=0.1)\n",
            "plt.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, color=\"g\", alpha=0.1)\n",
            "plt.title(\"Learning Curve for Decision Tree Classifier\")\n",
            "plt.xlabel(\"Training Set Size\")\n",
            "plt.ylabel(\"Accuracy Score\")\n",
            "plt.legend(loc=\"best\")\n",
            "plt.grid(True)\n",
            "plt.show()\n",
            "\n",
            "# Fit the model on the entire training dataset\n",
            "clf.fit(X, y)\n",
            "\n",
            "# Preprocess the test data and run predictions\n",
            "X_test, test_ids = preprocess_data(test_df, is_train=False)\n",
            "predictions = clf.predict(X_test)\n",
            "\n",
            "# Print the predictions along with PassengerId\n",
            "results = pd.DataFrame({'PassengerId': test_ids, 'Survived': predictions})\n",
            "print(results)\n",
            "Running code interpreter...\n",
            "[Code Interpreter] Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/site-packages (0.13.2)\n",
            "\n",
            "[Code Interpreter] Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "\n",
            "[Code Interpreter] Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.55.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
            "\n",
            "[Code Interpreter] Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
            "\n",
            "[Code Interpreter] \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "[Code Interpreter] \n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "[Code Interpreter ERROR] ExecutionError(name='ValueError', value='The feature names should match those that were passed during fit.\\nFeature names unseen at fit time:\\n- PassengerId\\n', traceback='---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)Cell In[1], line 84\\n     82 # Preprocess the test data and run predictions\\n     83 X_test, test_ids = preprocess_data(test_df, is_train=False)\\n---> 84 predictions = clf.predict(X_test)\\n     86 # Print the predictions along with PassengerId\\n     87 results = pd.DataFrame({\\'PassengerId\\': test_ids, \\'Survived\\': predictions})\\nFile /usr/local/lib/python3.10/site-packages/sklearn/tree/_classes.py:529, in BaseDecisionTree.predict(self, X, check_input)\\n    506 \"\"\"Predict class or regression value for X.\\n    507 \\n    508 For a classification model, the predicted class for each sample in X is\\n   (...)\\n    526     The predicted classes, or the predict values.\\n    527 \"\"\"\\n    528 check_is_fitted(self)\\n--> 529 X = self._validate_X_predict(X, check_input)\\n    530 proba = self.tree_.predict(X)\\n    531 n_samples = X.shape[0]\\nFile /usr/local/lib/python3.10/site-packages/sklearn/tree/_classes.py:489, in BaseDecisionTree._validate_X_predict(self, X, check_input)\\n    487 else:\\n    488     force_all_finite = True\\n--> 489 X = self._validate_data(\\n    490     X,\\n    491     dtype=DTYPE,\\n    492     accept_sparse=\"csr\",\\n    493     reset=False,\\n    494     force_all_finite=force_all_finite,\\n    495 )\\n    496 if issparse(X) and (\\n    497     X.indices.dtype != np.intc or X.indptr.dtype != np.intc\\n    498 ):\\n    499     raise ValueError(\"No support for np.int64 index based sparse matrices\")\\nFile /usr/local/lib/python3.10/site-packages/sklearn/base.py:608, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\\n    537 def _validate_data(\\n    538     self,\\n    539     X=\"no_validation\",\\n   (...)\\n    544     **check_params,\\n    545 ):\\n    546     \"\"\"Validate input data and set or check the `n_features_in_` attribute.\\n    547 \\n    548     Parameters\\n   (...)\\n    606         validated.\\n    607     \"\"\"\\n--> 608     self._check_feature_names(X, reset=reset)\\n    610     if y is None and self._get_tags()[\"requires_y\"]:\\n    611         raise ValueError(\\n    612             f\"This {self.__class__.__name__} estimator \"\\n    613             \"requires y to be passed, but the target y is None.\"\\n    614         )\\nFile /usr/local/lib/python3.10/site-packages/sklearn/base.py:535, in BaseEstimator._check_feature_names(self, X, reset)\\n    530 if not missing_names and not unexpected_names:\\n    531     message += (\\n    532         \"Feature names must be in the same order as they were in fit.\\\\n\"\\n    533     )\\n--> 535 raise ValueError(message)\\nValueError: The feature names should match those that were passed during fit.\\nFeature names unseen at fit time:\\n- PassengerId\\n')\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "No code interpreter results",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     first_result \u001b[38;5;241m=\u001b[39m code_results[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo code interpreter results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m first_result\n",
            "\u001b[0;31mException\u001b[0m: No code interpreter results"
          ]
        }
      ],
      "source": [
        "from e2b_code_interpreter import Sandbox\n",
        "\n",
        "with Sandbox(api_key=E2B_API_KEY) as code_interpreter:\n",
        "  # Upload the dataset to the code interpreter sandbox\n",
        "  upload_dataset(code_interpreter)\n",
        "\n",
        "  code_results = chat(\n",
        "    code_interpreter,\n",
        "    \"Clean the data, train a decision tree to predict the survival of passengers, and visualize the learning curve. Then run the model the test dataset and print the results.\"\n",
        "  )\n",
        "  if code_results:\n",
        "    first_result = code_results[0]\n",
        "  else:\n",
        "    raise Exception(\"No code interpreter results\")\n",
        "\n",
        "first_result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
